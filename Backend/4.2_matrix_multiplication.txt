#include <stdio.h>

__global__ void matmul(float *A, float *B, float *C, int N)
{
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if(row < N && col < N)
    {
        float sum = 0;
        for(int k=0; k<N; k++)
            sum = sum + A[row * N + k] * B[N * k + col];
        C[row * N + col] = sum;

    }
}

int main()
{
    int N = 2;
    size_t size = N * N * sizeof(float);
    float A[] = {1,2,3,4};
    float B[] = {5,6,7,8};
    float C[4];

    float *d_A, *d_B, *d_C;

    cudaMalloc(&d_A,size);
    cudaMalloc(&d_B,size);
    cudaMalloc(&d_C,size);

    cudaMemcpy(d_A,A,size,cudaMemcpyHostToDevice);
    cudaMemcpy(d_B,B,size,cudaMemcpyHostToDevice);

    dim3 blocks(N,N);
    dim3 threads(1,1);

    matmul<<<blocks,threads>>>(d_A,d_B,d_C,N);

    cudaMemcpy(C,d_C,size,cudaMemcpyDeviceToHost);

    for(int i = 0; i < N * N; i++)
        printf("%f ",C[i]);
}

OUTPUT :
The code starts with #include <stdio.h>, a preprocessor directive that includes the standard input/output library in C, providing functions like printf for printing to the console. The line __global__ void matmul(float *A, float *B, float *C, int N) declares a global function named matmul. The __global__ keyword is a CUDA extension, indicating that this function will execute on the GPU (device) and can be called from the CPU (host). This function takes four arguments: pointers to two float arrays A and B representing the input matrices, a pointer to a float array C where the result (product matrix) will be stored, and an integer N representing the dimension of the square matrices (N x N). Inside the matmul function, int row = blockIdx.y * blockDim.y + threadIdx.y; calculates the row index for the current thread. blockIdx.y gives the y-index of the current block in the grid, blockDim.y gives the number of threads per block in the y-dimension, and threadIdx.y gives the y-index of the current thread within the block. Similarly, int col = blockIdx.x * blockDim.x + threadIdx.x; calculates the column index for the current thread using blockIdx.x, blockDim.x, and threadIdx.x for the x-dimension. The if(row < N && col < N) statement checks if the calculated row and column indices are within the valid bounds of the N x N matrices. This ensures that each thread operates on a valid element of the output matrix. If the condition is true, float sum = 0; initializes a floating-point variable sum to 0. This variable will accumulate the dot product of the corresponding row of matrix A and the column of matrix B. The for(int k=0; k<N; k++) loop iterates N times, corresponding to the inner dimension of the matrix multiplication. Inside the loop, sum = sum + A[row * N + k] * B[N * k + col]; performs the multiplication of the element at A[row][k] (which is accessed as A[row * N + k] in a flattened 1D array representation of a row-major matrix) and the element at B[k][col] (accessed as B[N * k + col]), and adds the result to the sum. After the inner loop completes, C[row * N + col] = sum; stores the calculated sum, which is the element at the [row][col] position of the resulting matrix, into the C array (again using the flattened 1D representation).

The int main() function is the entry point of the CPU (host) code. int N = 2; declares an integer variable N and initializes it to 2, indicating that we are working with 2x2 matrices. size_t size = N * N * sizeof(float); calculates the total size in bytes required to store an N x N matrix of floats. float A[] = {1,2,3,4}; declares a float array A on the host and initializes it with the elements of the first 2x2 matrix in row-major order ([1 2], [3 4]). float B[] = {5,6,7,8}; declares a float array B on the host and initializes it with the elements of the second 2x2 matrix ([5 6], [7 8]). float C[4]; declares a float array C on the host with a size of 4, which will store the elements of the resulting 2x2 matrix after being copied back from the GPU. float *d_A, *d_B, *d_C; declares three float pointers d_A, d_B, and d_C. These pointers will point to memory allocated on the GPU (device). cudaMalloc(&d_A,size); allocates size bytes of memory on the GPU and assigns the starting address to the pointer d_A. Similarly, cudaMalloc(&d_B,size); and cudaMalloc(&d_C,size); allocate memory on the GPU for matrices B and C respectively. cudaMemcpy(d_A,A,size,cudaMemcpyHostToDevice); copies size bytes of data from the host array A to the device memory pointed to by d_A. The cudaMemcpyHostToDevice argument specifies the direction of the copy. cudaMemcpy(d_B,B,size,cudaMemcpyHostToDevice); does the same for array B, copying it from the host to the device memory pointed to by d_B. dim3 blocks(N,N); declares a 2D grid of blocks for the GPU execution. Here, it creates an N x N grid of blocks. Since N is 2, this means there will be 2x2 = 4 blocks. dim3 threads(1,1); declares the dimensions of each block of threads. Here, each block will contain 1 thread in the x-dimension and 1 thread in the y-dimension, so each block has only one thread. matmul<<<blocks,threads>>>(d_A,d_B,d_C,N); launches the matmul kernel on the GPU with the specified grid of blocks and threads per block. Each block (and its single thread) will be responsible for calculating one element of the resulting matrix C. cudaMemcpy(C,d_C,size,cudaMemcpyDeviceToHost); copies size bytes of data from the device memory pointed to by d_C back to the host array C. The cudaMemcpyDeviceToHost argument specifies the direction of the copy. The for(int i = 0; i < N * N; i++) printf("%f ",C[i]); loop iterates through all N * N elements of the host array C (which now contains the result of the matrix multiplication) and prints each element followed by a space using the printf function. Finally, the } closes the main function, and the program terminates.