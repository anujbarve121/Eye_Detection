#include <stdio.h>

__global__ void vector(float *A, float *B, float *C, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < N)
    {
        C[i] = A[i] + B[i];
    }
}

int main()
{
    int N = 4;
    size_t size = N * sizeof(float);
    float A[] = {1,2,3,4};
    float B[] = {5,6,7,8};
    float C[4];

    float *d_A, *d_B, *d_C;

    cudaMalloc(&d_A,size);
    cudaMalloc(&d_B,size);
    cudaMalloc(&d_C,size);

    cudaMemcpy(d_A,A,size,cudaMemcpyHostToDevice);
    cudaMemcpy(d_B,B,size,cudaMemcpyHostToDevice);

    vector<<<1,N>>>(d_A,d_B,d_C,N);

    cudaMemcpy(C,d_C,size,cudaMemcpyDeviceToHost);

    for(int i = 0; i < N; i++)
        printf("%f ",C[i]);
}

Theory :
The code begins with #include <stdio.h>, a preprocessor directive that includes the standard input/output library in C. This library provides functions like printf for printing output to the console. The line __global__ void vector(float *A, float *B, float *C, int N) declares a global function named vector. The __global__ keyword is a CUDA extension, indicating that this function will execute on the GPU (device) and can be called from the CPU (host). This function takes four arguments: pointers to two float arrays A and B, a pointer to a float array C where the result will be stored, and an integer N representing the number of elements to process. Inside the vector function, int i = blockIdx.x * blockDim.x + threadIdx.x; calculates the global thread ID i for the current thread executing on the GPU. blockIdx.x gives the index of the current block in the grid, blockDim.x gives the number of threads per block in the x-dimension, and threadIdx.x gives the index of the current thread within the block in the x-dimension. This formula maps each thread to a unique index in the overall data. The if(i < N) statement checks if the calculated global thread ID i is within the bounds of the number of elements N. This ensures that threads only operate on valid data indices. If the condition is true, C[i] = A[i] + B[i]; performs the element-wise addition of the i-th element of array A and the i-th element of array B, and stores the result in the i-th element of array C.

The int main() function is the entry point of the CPU (host) code. int N = 4; declares an integer variable N and initializes it to 4, representing the number of elements in the vectors. size_t size = N * sizeof(float); calculates the total size in bytes required to store N float elements. sizeof(float) gives the size of a single float in bytes, and this is multiplied by N. float A[] = {1,2,3,4}; declares a float array A on the host and initializes it with the values 1.0, 2.0, 3.0, and 4.0. float B[] = {5,6,7,8}; declares another float array B on the host and initializes it with the values 5.0, 6.0, 7.0, and 8.0. float C[4]; declares a float array C on the host with a size of 4, which will store the results copied back from the GPU. float *d_A, *d_B, *d_C; declares three float pointers d_A, d_B, and d_C. These pointers will point to memory allocated on the GPU (device). cudaMalloc(&d_A,size); allocates size bytes of memory on the GPU and assigns the starting address to the pointer d_A. Similarly, cudaMalloc(&d_B,size); and cudaMalloc(&d_C,size); allocate memory on the GPU for arrays B and C respectively. cudaMemcpy(d_A,A,size,cudaMemcpyHostToDevice); copies size bytes of data from the host array A to the device memory pointed to by d_A. The cudaMemcpyHostToDevice argument specifies the direction of the copy. cudaMemcpy(d_B,B,size,cudaMemcpyHostToDevice); does the same for array B, copying it from the host to the device memory pointed to by d_B. vector<<<1,N>>>(d_A,d_B,d_C,N); launches the vector kernel on the GPU. The <<<1,N>>> part specifies the execution configuration: 1 block in the grid and N threads per block. This means that N threads will be launched in parallel to perform the vector addition. d_A, d_B, d_C, and N are passed as arguments to the kernel function. cudaMemcpy(C,d_C,size,cudaMemcpyDeviceToHost); copies size bytes of data from the device memory pointed to by d_C back to the host array C. The cudaMemcpyDeviceToHost argument specifies the direction of the copy. The for(int i = 0; i < N; i++) printf("%f ",C[i]); loop iterates through the elements of the host array C (which now contains the results from the GPU) and prints each element followed by a space using the printf function. Finally, the } closes the main function, and the program terminates.